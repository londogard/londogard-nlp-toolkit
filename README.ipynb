{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "forbidden-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "@file:Repository(\"https://jitpack.io\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "equivalent-suspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "@file:DependsOn(\"com.londogard:londogard-nlp-toolkit:2505617f21\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-assistant",
   "metadata": {},
   "source": [
    "# Londogard NLP Toolkit\n",
    "This is a simple toolkit for Natural Language Processing (NLP) which will contain utilities that are very handy while prototyping but also for production deployment because of smart utilisation of resources.  \n",
    "The initial aim is not to solve problems end-2-end but rather have a simple small dependency with great utilities.\n",
    "\n",
    "## Supported Utilities\n",
    "\n",
    "All completed utilities with simple usage accompanied. `LanguageSupport.<ISO_2_COUNTRY_CODE>` helps figuring out what is supported.\n",
    "\n",
    "- [Word Embeddings](#WordEmbeddings) including basic Word Embeddings, 'Light Word Embeddings' & BytePairEmbeddings\n",
    "    - The Embeddings include automated downloads of languages to simplify your life, unless a path is specified.\n",
    "    - WordEmbeddings include 157 languages via `fastText`-embeddings ([fastText.cc](https://fasttext.cc/docs/en/crawl-vectors.html))\n",
    "    - BPE-Embeddings include 275 languages via `BPEmb`-embeddings ([nlp.h-its.org](https://nlp.h-its.org/bpemb/))\n",
    "    - All embeddings support ∞ languages through your own self-trained embeddings if a file-path is supplied!\n",
    "- [Sentence Embeddings](#SentenceEmbeddings) including `AvgSentenceEmbeddings` & `USifEmbeddings`\n",
    "- [Tokenizers](#Tokenizers) including Word, Char & Subword (SentencePiece) tokenizers (simple to add custom logic)\n",
    "    - SentencePiece include 275 languages via `BPEmb`-embeddings ([nlp.h-its.org](https://nlp.h-its.org/bpemb/)) with 8 vocab-sizes (1000, 3000, 10_000, 25_000, 50_000, 100_000, 200_000).\n",
    "        - Of course possible to supply your own tokenizer if you've a path to a trained one!\n",
    "- [Stopwords](#Stopwords) based on NLTKs list\n",
    "    - Supporting: ar, az, da, de, el, en, es, fi, fr, hu, id, it, kk, ne, nl, no, pt, ro, ru, sl, sv, tg & tr\n",
    "- [Word Frequencies](#WordFrequencies) based on `wordfreq.py` by [LuminosoInsight](https://github.com/LuminosoInsight/wordfreq/).\n",
    "    - Supporting: ar, cs, de, en, es, fi, fr, it, ja, nl, pl, uk, pt, ru, zh, bg, bn, ca, da, el, fa, he, hi, hu, id, ko, lv, mk, ms, nb, ro, sh, sv & tr. Some of these support \"Large Word Frequency\", call `LanguageSupport.<LANG_CODE>.largestWordFrequency()` to see if `large` variant is available.\n",
    "- [Stemmer](#Stemmer) based on [Snowball Stemmer](https://snowballstem.org/)\n",
    "    - Supporting: sv, nl, en, fi, fr, de, hu, it, no, pt, ro, ru, es & tr\n",
    "- [Trie](#Trie) - Just a basic utility that is to be used for a custom SubwordTokenizer in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-timeline",
   "metadata": {},
   "source": [
    "### WordEmbeddings\n",
    "Word Embeddings currently exists in three variants. \n",
    "\n",
    "All Embeddings (excluding SentenceEmbeddings) extend `Embeddings` which have some good-to-know default methods:\n",
    "```kotlin\n",
    "interface Embeddings {\n",
    "    fun contains(word: String): Boolean\n",
    "    \n",
    "    fun vector(word: String): SimpleMatrix?\n",
    "    \n",
    "    // OBS: Will return all possible vectors, not necessarily ALL\n",
    "    fun traverseVectors(words: List<String>): List<SimpleMatrix>\n",
    "    fun traverseVectorsOrNull(words: List<String>): List<SimpleMatrix>? // Returns null if any missing\n",
    "    \n",
    "    fun euclideanDistance(w1: String, w2: String): Double?\n",
    "    fun cosineDistance(w1: String, w2: String): Double?\n",
    "}\n",
    "```\n",
    "\n",
    "#### WordEmbedding\n",
    "\n",
    "`WordEmbeddings` are the classical usecase of Embeddings where each word maps to a vector of floats. There exists some helper-methods. Currently requires to have the embeddings locally. \n",
    "Download functions for [fastText](https://fasttext.cc/) will come, but be warned they're large!\n",
    "\n",
    "```kotlin\n",
    "class WordEmbeddings(val delimeter: Char = ' ', val dimensions: Int, val filePath: Path) {\n",
    "    // Returns the N nearest neighbours\n",
    "    fun nearestNeighbour(vector: SimpleMatrix, N: Int): List<Pair<String, Double>>\n",
    "    \n",
    "    // Returns N nearest neighbours, for the average of all input\n",
    "    fun distance(input: List<String>, N: Int): List<Pair<String, Double>>\n",
    "    \n",
    "    // w1 is to w2 what w3 is to ??. The N closest choices to ?? is selected\n",
    "    fun analogy(w1: String, w2: String, w3: String, N: Int): List<Pair<String, Double>>?\n",
    "    \n",
    "    // Rank a set of words by their distance to word\n",
    "    fun rank(word: String, set: Set<String>): List<Pair<String, Double>>\n",
    "    \n",
    "    // Pretty print the returned wordlist\n",
    "    fun pprint(words: List<Pair<String, Double>>)\n",
    "}\n",
    "```\n",
    "\n",
    "#### LightWordEmbedding\n",
    "\n",
    "`LightWordEmbeddings` is something we at Londogard created to allow our embeddings to be loaded onto a Raspberry Pi 3B+ (1GB RAM). What's so effective about the `LightWordEmbeddings` is that ~ 10 % of all words makes up 90 % of all communications meaning that by just having a few embeddings (the most common ones) we cover most cases and can load the rest when required.  \n",
    "They don't come free as you can't call the unique functions from `WordEmbeddings` such as `nearestNeighbour` and are a little bit more complicated to use.\n",
    "\n",
    "```kotlin\n",
    "class LightWordEmbeddings(val delimeter: Char = ' ', val dimensions: Int, val filePath: Path, val maxWordCount: Int = 1000) {\n",
    "    \n",
    "    // add words you'd like to read into the embeddings.\n",
    "    // only delta from already loaded words are added, e.g. if everything is loaded it won't head to filesystem\n",
    "    // the `maxWordCount` most common words are preloaded as part of instatiation\n",
    "    fun addWords(words: Set<String) \n",
    "}\n",
    "```\n",
    "\n",
    "#### BytePieceEncoding-Embedding (BPEmb)\n",
    "BytePieceEncoding-Embeddings are a new approach from _BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages_ by Benjamin Heinzerling and Michael Strube. In this paper they show how a 11 MB embedding for English is on par with 6 GB embeddings from `fastText`!\n",
    "\n",
    "The default behaviour is that the tokenizer tokenize a word and average the embedding. It's also possible to use the extended method `subwordVector` to directly retrieve the embedding if you pretokenized the text. Remember if using `subwordVector` it's important that you choose the same vocab-size, otherwise you'll see a lot of misses!\n",
    "\n",
    "```kotlin\n",
    "class BpeEmbeddings {\n",
    "    fun subwordVector(subword: String): SimpleMatrix?\n",
    "}\n",
    "```\n",
    "\n",
    "#### EmbeddingLoader\n",
    "Finally there exists a utility function that helps you load & download embeddings automatically and saving them on filesystem. \n",
    "\n",
    "Let's see how we can use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "innovative-reminder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/londogard/.londogard/bpe/sv.wiki.bpe.vs10000.vocab\n",
      "/home/londogard/.londogard/bpe/sv.wiki.bpe.vs10000.model\n",
      "Type = FDRM , rows = 1 , cols = 10\n",
      "-7.9100E-02 -4.8000E-02 -1.7440E-01  1.1110E-01 -6.3600E-02 -2.3520E-01 -5.4400E-02  1.1200E-01 -4.0000E-04 -1.5900E-02 \n",
      "\n",
      "Loading LightWordEmbedding + retrieving 'Hej' took 50 milliseconds\n"
     ]
    }
   ],
   "source": [
    "import com.londogard.nlp.embeddings.*\n",
    "import com.londogard.nlp.utils.LanguageSupport.*\n",
    "import kotlin.system.measureTimeMillis\n",
    "\n",
    "EmbeddingLoader.fromLanguageOrNull<WordEmbeddings>(sv) // WordEmbeddings\n",
    "EmbeddingLoader.fromLanguageOrNull<BpeEmbeddings>(sv) // BpeEmbeddings (vocabSize: 10_000, dim: 50)\n",
    "EmbeddingLoader.fromLanguageOrNull<LightWordEmbeddings>(sv) // LightWordEmbeddings (size: 1000)\n",
    "\n",
    "measureTimeMillis {\n",
    "    val embeddings = EmbeddingLoader.fromLanguageOrNull<LightWordEmbeddings>(sv)\n",
    "    println(embeddings?.vector(\"Hej\")?.cols(0, 10)) // Trunctating 300 dimensions to 10 to make print nicer\n",
    "}.let { ms -> println(\"Loading LightWordEmbeddings + retrieving 'Hej' took $ms milliseconds\") }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-toronto",
   "metadata": {},
   "source": [
    "`LightWordEmbeddings` are really good to keep memory requirements at low and having a quick boot-up from cold. `WordEmbeddings` are obviously faster once running \"hot\" but it might not be possible if your RAM is too low (e.g. running on embedded hardware, Raspberry Pi etc).\n",
    "\n",
    "See how long `WordEmbeddings` take to boot up in comarison!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "subsequent-peeing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type = FDRM , rows = 1 , cols = 10\n",
      "-7.9100E-02 -4.8000E-02 -1.7440E-01  1.1110E-01 -6.3600E-02 -2.3520E-01 -5.4400E-02  1.1200E-01 -4.0000E-04 -1.5900E-02 \n",
      "\n",
      "Loading WordEmbeddings + retrieving 'Hej' took 65006 milliseconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65006"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "new_classpath": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measureTimeMillis {\n",
    "    val embeddings = EmbeddingLoader.fromLanguageOrNull<WordEmbeddings>(sv)\n",
    "    println(embeddings?.vector(\"Hej\")?.cols(0, 10)) // Trunctating 300 dimensions to 10 to make print nicer\n",
    "}.let { ms -> println(\"Loading WordEmbeddings + retrieving 'Hej' took $ms milliseconds\") }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-lawsuit",
   "metadata": {},
   "source": [
    "And finally we have `BpeEmbeddings` which I believe is the best out of all. They combine speed, RAM and eveything into one great package. The bootup is a little bit slower than `LightWordEmbeddings` but it is possible to keep the full Embedding in memory on low-memory hardware because of the great sizes meaning that it'll keep being incredibly fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "completed-chosen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/londogard/.londogard/bpe/sv.wiki.bpe.vs10000.vocab\n",
      "/home/londogard/.londogard/bpe/sv.wiki.bpe.vs10000.model\n",
      "Type = FDRM , rows = 1 , cols = 10\n",
      " 4.1733E-02  1.0365E-01  9.4801E-02  3.9651E-02  3.2153E-01  3.3000E-02 -2.7866E-01  1.6301E-01 -2.5834E-01  7.7778E-02 \n",
      "\n",
      "Loading BpeEmbeddings + retrieving 'Hej' took 446 milliseconds\n"
     ]
    }
   ],
   "source": [
    "measureTimeMillis {\n",
    "    val embeddings = EmbeddingLoader.fromLanguageOrNull<BpeEmbeddings>(sv)\n",
    "    println(embeddings?.vector(\"Hej\")?.cols(0, 10)) // Trunctating 300 dimensions to 10 to make print nicer\n",
    "}.let { ms -> println(\"Loading BpeEmbeddings + retrieving 'Hej' took $ms milliseconds\") }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-crash",
   "metadata": {},
   "source": [
    "### SentenceEmbeddings\n",
    "\n",
    "All Sentence Embeddings extend the same `interface`\n",
    "```kotlin\n",
    "interface SentenceEmbeddings {\n",
    "    val tokenEmbeddings: Embeddings\n",
    "    fun getSentenceEmbeddings(listOfSentences: List<List<String>>): List<SimpleMatrix>\n",
    "    fun getSentenceEmbedding(sentence: List<String>): SimpleMatrix\n",
    "}\n",
    "```\n",
    "\n",
    "The `fun getSentenceEmbeddings(listOfSentences: List<List<String>>)` exists because some Sentence Embeddings depends on the \"global context\".\n",
    "\n",
    "Currently two variants are 'completed' but more are coming.\n",
    "\n",
    "#### AvgSentenceEmbeddings\n",
    "Just averages the Word Embeddings for a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "operating-overhead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type = FDRM , rows = 1 , cols = 10\n",
       "-2.3792E-02 -2.1282E-02 -4.2597E-02  3.0897E-02 -2.5323E-02 -9.7709E-02 -1.1211E-02  3.5199E-02  1.2124E-02 -8.6041E-03 \n"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "new_classpath": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.londogard.nlp.embeddings.sentence.*\n",
    "\n",
    "val embeddings = EmbeddingLoader.fromLanguageOrNull<LightWordEmbeddings>(sv)!! // We know this exists.. :)\n",
    "val sentEmbeddings = AverageSentenceEmbeddings(embeddings)\n",
    "\n",
    "sentEmbeddings.getSentenceEmbedding(listOf(\"Hej\", \"där\", \"borta\")).cols(0, 10) // once again reducing dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-creator",
   "metadata": {},
   "source": [
    "#### USifSentenceEmbeddings\n",
    "This implementation is based on the paper _Unsupervised Random Walk Sentence Embeddings: A Strong but Simple Baseline_ by Kawin Ethayarajh, found on [aclweb.org](https://www.aclweb.org/anthology/W18-3012/).  \n",
    "This paper bases its work on Smooth-Inverse-Frequency (SIF) Embeddings (paper _A Simple but Tough-to-Beat Baseline for Sentence Embeddings_ found [here](https://openreview.net/forum?id=SyK00v5xx)). The difference being that this approach is _unsupervised_ while remaining even stronger. See the abstract:\n",
    "\n",
    "> Using a random walk model of text generation, Arora et al. (2017) proposed a strong baseline for computing sentence embeddings: take a weighted average of word embeddings and modify with SVD. This simple method even outperforms far more complex approaches such as LSTMs on textual similarity tasks. In this paper, we first show that word vector length has a confounding effect on the probability of a sentence being generated in Arora et al.’s model. We propose a random walk model that is robust to this confound, where the probability of word generation is inversely related to the angular distance between the word and sentence embeddings. Our approach beats Arora et al.’s by up to 44.4% on textual similarity tasks and is competitive with state-of-the-art methods. Unlike Arora et al.’s method, ours requires no hyperparameter tuning, which means it can be used when there is no labelled data.\n",
    "\n",
    "What has to be noted is that it requires more input than other `SentenceEmbeddings` see\n",
    "\n",
    "```kotlin\n",
    "import com.londogard.nlp.embeddings.sentence.*\n",
    "\n",
    "class USifSentenceEmbeddings(\n",
    "    val tokenEmbeddings: Embeddings,\n",
    "    private val wordProb: Map<String, Float>,\n",
    "    randomWalkLength: Int, // = n, ~11\n",
    "    val numCommonDiscourseVector: Int = 5 // = m, 0 should work. In practise max 5.\n",
    ") {\n",
    "    /** use it the same way as SentenceEmbeddings */\n",
    "}\n",
    "```\n",
    "\n",
    "Where `wordProb` is simply taken through the `WordFrequencies` util. E.g. `WordFrequencies.getAllWordFrequenciesOrNull(sv): Map<String, Float>?`.\n",
    "\n",
    "#### SentenceEmbeddings In Progress\n",
    "\n",
    "`TfIdfEmbeddings` & `SifEmbeddings`, the latter might be scrapped as `USif` is such a amazing work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "major-mitchell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type = FDRM , rows = 1 , cols = 10\n",
       "-3.1692E-02 -2.7498E-02 -5.7967E-02  4.1469E-02 -3.2962E-02 -1.2680E-01 -1.5575E-02  4.6698E-02  1.4627E-02 -1.0986E-02 \n"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "new_classpath": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.londogard.nlp.wordfreq.WordFrequencies\n",
    "\n",
    "val usifEmbeddings = USifSentenceEmbeddings(embeddings, WordFrequencies.getAllWordFrequenciesOrNull(sv) ?: emptyMap(), 11)\n",
    "usifEmbeddings.getSentenceEmbedding(listOf(\"Hej\", \"där\", \"borta\")).cols(0, 10) // Also trunctating here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-short",
   "metadata": {},
   "source": [
    "### Tokenizers\n",
    "\n",
    "There currently exists three types of tokenizers all extending the same `interface`.\n",
    "\n",
    "```kotlin\n",
    "interface Tokenizer {\n",
    "    fun split(text: String): List<String>\n",
    "}\n",
    "```\n",
    "\n",
    "#### CharTokenizer\n",
    "\n",
    "Tokenizes a string into the chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "spatial-iceland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[h, e, l, l, o, ,,  , w, o, r, l, d, !]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "new_classpath": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.londogard.nlp.tokenizer.*\n",
    "\n",
    "CharTokenizer().split(\"hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-russia",
   "metadata": {},
   "source": [
    "#### SimpleTokenizer\n",
    "\n",
    "This is a word-tokenizer which splits out words based on a few different "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "animated-aging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[hello, ,, world, !]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "new_classpath": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SimpleTokenizer().split(\"hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-queen",
   "metadata": {},
   "source": [
    "#### SentencePieceTokenizer\n",
    "The SentencePiece Tokenizer is a subword tokenizer which is Language Specific, through [BPEmb](https://nlp.h-its.org/bpemb/) we have 275 languages covered through Wikipedia. There exists model of the following vocab-sizes: `1000, 3000, 5000, 10_000, 25_000, 50_000, 100_000 & 200_000`. The larger vocab the less subwords are tokenized and more words.\n",
    "\n",
    "The SentencePiece model is the raw C++ from [Google](https://github.com/google/sentencepiece/) with a wrapper from [DJL (Amazon)](http://docs.djl.ai/extensions/sentencepiece/index.html). I'm usually very hesitant in adding native libraries (JNI) when working on JVM-projects but no-one can deny the power of SentencePiece and I haven't had the time to implement the algorithm myself.  \n",
    "This wrapper by DJL is very small (they provide a single dependency with only sentencepiece) and fits my philosophy of keeping dependencies and size at a low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "extensive-ferry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/londogard/.londogard/bpe/sv.wiki.bpe.vs10000.vocab\n",
      "/home/londogard/.londogard/bpe/sv.wiki.bpe.vs10000.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[▁he, j, ▁där, ▁bor, ta, ,, ▁hur, ▁mår, ▁ni, ?]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "new_classpath": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentencePieceTokenizer.fromLanguageSupportOrNull(sv)?.split(\"hej där borta, hur mår ni?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-yugoslavia",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "The stopwords are taken from NLTK and are hosted directly on the GitHub. The object looks as follows:\n",
    "\n",
    "```kotlin\n",
    "object Stopwords {\n",
    "    fun isStopword(word: String, language: LanguageSupport): Boolean\n",
    "    fun stopwords(language: LanguageSupport): Set<String> // Throws if language does not support stopwords\n",
    "    fun stopwordsOrNull(language: LanguageSupport): Set<String>?\n",
    "}\n",
    "```\n",
    "\n",
    "This object has an internal cache which saves the previously loaded language. Use `Stopwords.stopwords` to simply retrieve the Stopwords and use them yourself as a `Set<String>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "united-response",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hej' is a stopword: false\n",
       "'och' is a stopword: true"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "new_classpath": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.londogard.nlp.stopwords.Stopwords\n",
    "import com.londogard.nlp.utils.LanguageSupport.*\n",
    "\n",
    "val hej = Stopwords.isStopword(\"hej\", sv)\n",
    "val och = Stopwords.isStopword(\"och\", sv)\n",
    "\n",
    "\"'hej' is a stopword: $hej\\n'och' is a stopword: $och\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "armed-colleague",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[och, det, att]"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "new_classpath": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Stopwords.stopwords(sv).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "persistent-savage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "new_classpath": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runCatching { Stopwords.stopwords(af) }.isSuccess // Stopwords does not support af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cardiovascular-corporation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "new_classpath": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Stopwords.stopwordsOrNull(af) == null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-adaptation",
   "metadata": {},
   "source": [
    "### WordFrequencies\n",
    "\n",
    "The Word Frequencies are taken from `wordfreq.py` a library by [LuminosoInsight](https://github.com/LuminosoInsight/wordfreq/) and are hosted directly on the GitHub. The object looks as follows:\n",
    "\n",
    "```kotlin\n",
    "object WordFrequencies {\n",
    "   fun getAllWordFrequenciesOrNull(language: LanguageSupport, size: WordFrequencySize = WordFrequencySize.Largest): Map<String, Float>?\n",
    "   \n",
    "   fun wordFrequency(word: String, language: LanguageSupport, minimum: Float = 0f, size: WordFrequencySize): Float // Throws if language does not support wordfreq\n",
    "   fun wordFrequencyOrNull( word: String, language: LanguageSupport, minimum: Float = 0f, size: WordFrequencySize): Float?\n",
    "}\n",
    "```\n",
    "\n",
    "This object has an internal cache which saves the previously loaded language. Use `WordFrequencies.getAllWordFrequenciesOrNull` to simply retrieve the WordFrequencies and use them yourself as a `Map<String, Float>`.  \n",
    "Methods to recieve `zipfFrequencies` also exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "expensive-adjustment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordFrequency of 'hej'=2.9512093E-4 and 'och'=0.025118863"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "new_classpath": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.londogard.nlp.wordfreq.WordFrequencies\n",
    "\n",
    "val hej = WordFrequencies.wordFrequency(\"hej\", sv)\n",
    "val och = WordFrequencies.wordFrequency(\"och\", sv)\n",
    "\n",
    "\"WordFrequency of 'hej'=$hej and 'och'=$och\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "native-mystery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZipfFrequency of 'hej'=5.4700003 and 'och'=7.4"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "new_classpath": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hej = WordFrequencies.zipfFrequency(\"hej\", sv)\n",
    "val och = WordFrequencies.zipfFrequency(\"och\", sv)\n",
    "\n",
    "\"ZipfFrequency of 'hej'=$hej and 'och'=$och\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "russian-consistency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordFrequency of 'hraihaodjasmdiamo' (non-word) using `wordFrequency` 0.0 and using `wordFrequencyOrNull` null"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "new_classpath": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weird = WordFrequencies.wordFrequency(\"hraihaodjasmdiamo\", sv)\n",
    "val weirdOrNull = WordFrequencies.wordFrequencyOrNull(\"hraihaodjasmdiamo\", sv)\n",
    "\n",
    "\"WordFrequency of 'hraihaodjasmdiamo' (non-word) using `wordFrequency` $weird and using `wordFrequencyOrNull` $weirdOrNull\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "visible-message",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language af does not have (Smallest) word frequencies locally. Will download (few KBs)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "new_classpath": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runCatching { WordFrequencies.wordFrequency(\"hello\", af) }.isFailure // WordFrequencies does not support af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "objective-wright",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language af does not have (Smallest) word frequencies locally. Will download (few KBs)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java.io.FileNotFoundException: https://raw.githubusercontent.com/londogard/londogard-nlp-toolkit/main/data/wordfreq/small_af.gz\n",
      "java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1928)\n",
      "java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1528)\n",
      "java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getInputStream(HttpsURLConnectionImpl.java:224)\n",
      "java.base/java.net.URL.openStream(URL.java:1167)\n",
      "com.londogard.nlp.utils.DownloadHelper.saveTo(DownloadHelper.kt:112)\n",
      "com.londogard.nlp.utils.DownloadHelper.getWordFrequencies(DownloadHelper.kt:40)\n",
      "com.londogard.nlp.wordfreq.WordFrequencies.wordFrequencyOrNull(WordFrequencies.kt:51)\n",
      "com.londogard.nlp.wordfreq.WordFrequencies.wordFrequencyOrNull$default(WordFrequencies.kt:48)\n",
      "Line_1332_jupyter.<init>(Line_1332.jupyter.kts:1)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:64)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.evalWithConfigAndOtherScriptsResults(BasicJvmScriptEvaluator.kt:96)\n",
      "kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.invoke$suspendImpl(BasicJvmScriptEvaluator.kt:41)\n",
      "kotlin.script.experimental.jvm.BasicJvmScriptEvaluator.invoke(BasicJvmScriptEvaluator.kt)\n",
      "kotlin.script.experimental.jvm.BasicJvmReplEvaluator.eval(BasicJvmReplEvaluator.kt:51)\n",
      "org.jetbrains.kotlin.jupyter.ReplForJupyterImpl$doEval$resultWithDiagnostics$1.invokeSuspend(repl.kt:604)\n",
      "kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)\n",
      "kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:56)\n",
      "kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:274)\n",
      "kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:84)\n",
      "kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:59)\n",
      "kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source)\n",
      "kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:38)\n",
      "kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source)\n",
      "org.jetbrains.kotlin.jupyter.ReplForJupyterImpl.doEval(repl.kt:604)\n",
      "org.jetbrains.kotlin.jupyter.ReplForJupyterImpl.eval(repl.kt:434)\n",
      "org.jetbrains.kotlin.jupyter.ProtocolKt$shellMessagesHandler$res$1.invoke(protocol.kt:248)\n",
      "org.jetbrains.kotlin.jupyter.ProtocolKt$shellMessagesHandler$res$1.invoke(protocol.kt)\n",
      "org.jetbrains.kotlin.jupyter.ProtocolKt.evalWithIO(protocol.kt:400)\n",
      "org.jetbrains.kotlin.jupyter.ProtocolKt.shellMessagesHandler(protocol.kt:247)\n",
      "org.jetbrains.kotlin.jupyter.IkotlinKt.kernelServer(ikotlin.kt:127)\n",
      "org.jetbrains.kotlin.jupyter.IkotlinKt.kernelServer$default(ikotlin.kt:95)\n",
      "org.jetbrains.kotlin.jupyter.IkotlinKt.main(ikotlin.kt:72)\n"
     ]
    }
   ],
   "source": [
    "WordFrequencies.wordFrequencyOrNull(\"hello\", af) == null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "numerical-young",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[är=0.037153527, det=0.031622775, att=0.026302677]"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "new_classpath": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordFrequencies.getAllWordFrequenciesOrNull(sv)?.entries?.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-nomination",
   "metadata": {},
   "source": [
    "### Stemmer\n",
    "\n",
    "The stemmer uses [Snowballstem](http://snowballstem.org/) which is a small dependency with a wrapper.  \n",
    "If the stemmer is not supported by the called `LanguageSupport` it'll fall-back to the classic `PorterStemmer`.\n",
    "\n",
    "\n",
    "There exists two ways to call the stemmer currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "objective-twelve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "katt"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "new_classpath": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.londogard.nlp.stemmer.Stemmer\n",
    "\n",
    "val stemmer = Stemmer(sv)\n",
    "stemmer.stem(\"katten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "muslim-craft",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "katt"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "new_classpath": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Stemmer.stem(\"katten\", sv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-anger",
   "metadata": {},
   "source": [
    "### Trie\n",
    "\n",
    ":warning: Work In Progress :warning:\n",
    "\n",
    "Does work with a `vocab: Map<String, Int>`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "pygments_lexer": "kotlin",
   "version": "1.5.0-dev-1206"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}