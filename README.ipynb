{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@file:Repository(\"https://jitpack.io\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@file:DependsOn(\"com.londogard:londogard-nlp-toolkit:clf-SNAPSHOT\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Londogard NLP Toolkit\n",
    "This is a simple toolkit for Natural Language Processing (NLP) which will contain utilities that are very handy while prototyping but also for production deployment because of smart utilisation of resources.  \n",
    "The initial aim is not to solve problems end-2-end but rather have a simple small dependency with great utilities.\n",
    "\n",
    "## Supported Utilities\n",
    "\n",
    "All completed utilities with simple usage accompanied. `LanguageSupport.<ISO_2_COUNTRY_CODE>` helps figuring out what is supported.\n",
    "\n",
    "- [Word Embeddings](#WordEmbeddings) including basic Word Embeddings, 'Light Word Embeddings' & BytePairEmbeddings\n",
    "    - The Embeddings include automated downloads of languages to simplify your life, unless a path is specified.\n",
    "    - WordEmbeddings include 157 languages via `fastText`-embeddings ([fastText.cc](https://fasttext.cc/docs/en/crawl-vectors.html))\n",
    "    - BPE-Embeddings include 275 languages via `BPEmb`-embeddings ([nlp.h-its.org](https://nlp.h-its.org/bpemb/))\n",
    "    - All embeddings support ∞ languages through your own self-trained embeddings if a file-path is supplied!\n",
    "- [Sentence Embeddings](#SentenceEmbeddings) including `AvgSentenceEmbeddings` & `USifEmbeddings`\n",
    "- [Tokenizers](#Tokenizers) including Word, Char & Subword (SentencePiece) tokenizers (simple to add custom logic)\n",
    "    - SentencePiece include 275 languages via `BPEmb`-embeddings ([nlp.h-its.org](https://nlp.h-its.org/bpemb/)) with 8 vocab-sizes (1000, 3000, 10_000, 25_000, 50_000, 100_000, 200_000).\n",
    "        - Of course possible to supply your own tokenizer if you've a path to a trained one!\n",
    "- [Stopwords](#Stopwords) based on NLTKs list\n",
    "    - Supporting: ar, az, da, de, el, en, es, fi, fr, hu, id, it, kk, ne, nl, no, pt, ro, ru, sl, sv, tg & tr\n",
    "- [Word Frequencies](#WordFrequencies) based on `wordfreq.py` by [LuminosoInsight](https://github.com/LuminosoInsight/wordfreq/).\n",
    "    - Supporting: ar, cs, de, en, es, fi, fr, it, ja, nl, pl, uk, pt, ru, zh, bg, bn, ca, da, el, fa, he, hi, hu, id, ko, lv, mk, ms, nb, ro, sh, sv & tr. Some of these support \"Large Word Frequency\", call `LanguageSupport.<LANG_CODE>.largestWordFrequency()` to see if `large` variant is available.\n",
    "- [Stemmer](#Stemmer) based on [Snowball Stemmer](https://snowballstem.org/)\n",
    "    - Supporting: sv, nl, en, fi, fr, de, hu, it, no, pt, ro, ru, es & tr\n",
    "- [Trie](#Trie) - Just a basic utility that is to be used for a custom SubwordTokenizer in the future.\n",
    "- [Vectorizer](#Vectorizer) - Vectorizers & Transformers. Include BM25, TF-IDF & BagOfWords (CountVectorizer).\n",
    "- [Classifiers](#Classifiers) - Classifiers that predicts a class (label), includes Logisitc Regression & Naïve Bayes.\n",
    "- [Regression](#Regression) - Linear Regression which is a simple regression.\n",
    "- [Sequence Classifier](#SequenceClassifier) - Hidden Markov Model to predict sequences like Part of Speech (PoS).\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### WordEmbeddings\n",
    "Word Embeddings currently exists in three variants. \n",
    "\n",
    "All Embeddings (excluding SentenceEmbeddings) extend `Embeddings` which have some good-to-know default methods:\n",
    "```kotlin\n",
    "interface Embeddings {\n",
    "    fun contains(word: String): Boolean\n",
    "    \n",
    "    fun vector(word: String): SimpleMatrix?\n",
    "    \n",
    "    // OBS: Will return all possible vectors, not necessarily ALL\n",
    "    fun traverseVectors(words: List<String>): List<SimpleMatrix>\n",
    "    fun traverseVectorsOrNull(words: List<String>): List<SimpleMatrix>? // Returns null if any missing\n",
    "    \n",
    "    fun euclideanDistance(w1: String, w2: String): Double?\n",
    "    fun cosineDistance(w1: String, w2: String): Double?\n",
    "}\n",
    "```\n",
    "\n",
    "#### WordEmbedding\n",
    "\n",
    "`WordEmbeddings` are the classical usecase of Embeddings where each word maps to a vector of floats. There exists some helper-methods. Currently requires to have the embeddings locally. \n",
    "Download functions for [fastText](https://fasttext.cc/) will come, but be warned they're large!\n",
    "\n",
    "```kotlin\n",
    "class WordEmbeddings(val delimeter: Char = ' ', val dimensions: Int, val filePath: Path) {\n",
    "    // Returns the N nearest neighbours\n",
    "    fun nearestNeighbour(vector: SimpleMatrix, N: Int): List<Pair<String, Double>>\n",
    "    \n",
    "    // Returns N nearest neighbours, for the average of all input\n",
    "    fun distance(input: List<String>, N: Int): List<Pair<String, Double>>\n",
    "    \n",
    "    // w1 is to w2 what w3 is to ??. The N closest choices to ?? is selected\n",
    "    fun analogy(w1: String, w2: String, w3: String, N: Int): List<Pair<String, Double>>?\n",
    "    \n",
    "    // Rank a set of words by their distance to word\n",
    "    fun rank(word: String, set: Set<String>): List<Pair<String, Double>>\n",
    "    \n",
    "    // Pretty print the returned wordlist\n",
    "    fun pprint(words: List<Pair<String, Double>>)\n",
    "}\n",
    "```\n",
    "\n",
    "#### LightWordEmbedding\n",
    "\n",
    "`LightWordEmbeddings` is something we at Londogard created to allow our embeddings to be loaded onto a Raspberry Pi 3B+ (1GB RAM). What's so effective about the `LightWordEmbeddings` is that ~ 10 % of all words makes up 90 % of all communications meaning that by just having a few embeddings (the most common ones) we cover most cases and can load the rest when required.  \n",
    "They don't come free as you can't call the unique functions from `WordEmbeddings` such as `nearestNeighbour` and are a little bit more complicated to use.\n",
    "\n",
    "```kotlin\n",
    "class LightWordEmbeddings(val delimeter: Char = ' ', val dimensions: Int, val filePath: Path, val maxWordCount: Int = 1000) {\n",
    "    \n",
    "    // add words you'd like to read into the embeddings.\n",
    "    // only delta from already loaded words are added, e.g. if everything is loaded it won't head to filesystem\n",
    "    // the `maxWordCount` most common words are preloaded as part of instatiation\n",
    "    fun addWords(words: Set<String) \n",
    "}\n",
    "```\n",
    "\n",
    "#### BytePieceEncoding-Embedding (BPEmb)\n",
    "BytePieceEncoding-Embeddings are a new approach from _BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages_ by Benjamin Heinzerling and Michael Strube. In this paper they show how a 11 MB embedding for English is on par with 6 GB embeddings from `fastText`!\n",
    "\n",
    "The default behaviour is that the tokenizer tokenize a word and average the embedding. It's also possible to use the extended method `subwordVector` to directly retrieve the embedding if you pretokenized the text. Remember if using `subwordVector` it's important that you choose the same vocab-size, otherwise you'll see a lot of misses!\n",
    "\n",
    "```kotlin\n",
    "class BpeEmbeddings {\n",
    "    fun subwordVector(subword: String): SimpleMatrix?\n",
    "}\n",
    "```\n",
    "\n",
    "#### EmbeddingLoader\n",
    "Finally there exists a utility function that helps you load & download embeddings automatically and saving them on filesystem. \n",
    "\n",
    "Let's see how we can use this!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import com.londogard.nlp.embeddings.*\n",
    "import com.londogard.nlp.utils.LanguageSupport.*\n",
    "import kotlin.system.measureTimeMillis\n",
    "\n",
    "EmbeddingLoader.fromLanguageOrNull<WordEmbeddings>(sv) // WordEmbeddings\n",
    "EmbeddingLoader.fromLanguageOrNull<BpeEmbeddings>(sv) // BpeEmbeddings (vocabSize: 10_000, dim: 50)\n",
    "EmbeddingLoader.fromLanguageOrNull<LightWordEmbeddings>(sv) // LightWordEmbeddings (size: 1000)\n",
    "\n",
    "measureTimeMillis {\n",
    "    val embeddings = EmbeddingLoader.fromLanguageOrNull<LightWordEmbeddings>(sv)\n",
    "    println(embeddings?.vector(\"Hej\")?.cols(0, 10)) // Trunctating 300 dimensions to 10 to make print nicer\n",
    "}.let { ms -> println(\"Loading LightWordEmbeddings + retrieving 'Hej' took $ms milliseconds\") }"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type = FDRM , rows = 1 , cols = 10\n",
      "-7.9100E-02 -4.8000E-02 -1.7440E-01  1.1110E-01 -6.3600E-02 -2.3520E-01 -5.4400E-02  1.1200E-01 -4.0000E-04 -1.5900E-02 \n",
      "\n",
      "Loading LightWordEmbeddings + retrieving 'Hej' took 79 milliseconds\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`LightWordEmbeddings` are really good to keep memory requirements at low and having a quick boot-up from cold. `WordEmbeddings` are obviously faster once running \"hot\" but it might not be possible if your RAM is too low (e.g. running on embedded hardware, Raspberry Pi etc).\n",
    "\n",
    "See how long `WordEmbeddings` take to boot up in comarison!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "measureTimeMillis {\n",
    "    val embeddings = EmbeddingLoader.fromLanguageOrNull<WordEmbeddings>(sv)\n",
    "    println(embeddings?.vector(\"Hej\")?.cols(0, 10)) // Trunctating 300 dimensions to 10 to make print nicer\n",
    "}.let { ms -> println(\"Loading WordEmbeddings + retrieving 'Hej' took $ms milliseconds\") }"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type = FDRM , rows = 1 , cols = 10\n",
      "-7.9100E-02 -4.8000E-02 -1.7440E-01  1.1110E-01 -6.3600E-02 -2.3520E-01 -5.4400E-02  1.1200E-01 -4.0000E-04 -1.5900E-02 \n",
      "\n",
      "Loading WordEmbeddings + retrieving 'Hej' took 63439 milliseconds\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And finally we have `BpeEmbeddings` which I believe is the best out of all. They combine speed, RAM and eveything into one great package. The bootup is a little bit slower than `LightWordEmbeddings` but it is possible to keep the full Embedding in memory on low-memory hardware because of the great sizes meaning that it'll keep being incredibly fast!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "measureTimeMillis {\n",
    "    val embeddings = EmbeddingLoader.fromLanguageOrNull<BpeEmbeddings>(sv)\n",
    "    println(embeddings?.vector(\"Hej\")?.cols(0, 10)) // Trunctating 300 dimensions to 10 to make print nicer\n",
    "}.let { ms -> println(\"Loading BpeEmbeddings + retrieving 'Hej' took $ms milliseconds\") }"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type = FDRM , rows = 1 , cols = 10\n",
      " 4.1733E-02  1.0365E-01  9.4801E-02  3.9651E-02  3.2153E-01  3.3000E-02 -2.7866E-01  1.6301E-01 -2.5834E-01  7.7778E-02 \n",
      "\n",
      "Loading BpeEmbeddings + retrieving 'Hej' took 302 milliseconds\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SentenceEmbeddings\n",
    "\n",
    "All Sentence Embeddings extend the same `interface`\n",
    "```kotlin\n",
    "interface SentenceEmbeddings {\n",
    "    val tokenEmbeddings: Embeddings\n",
    "    fun getSentenceEmbeddings(listOfSentences: List<List<String>>): List<SimpleMatrix>\n",
    "    fun getSentenceEmbedding(sentence: List<String>): SimpleMatrix\n",
    "}\n",
    "```\n",
    "\n",
    "The `fun getSentenceEmbeddings(listOfSentences: List<List<String>>)` exists because some Sentence Embeddings depends on the \"global context\".\n",
    "\n",
    "Currently two variants are 'completed' but more are coming.\n",
    "\n",
    "#### AvgSentenceEmbeddings\n",
    "Just averages the Word Embeddings for a sentence."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import com.londogard.nlp.embeddings.sentence.*\n",
    "\n",
    "val embeddings = EmbeddingLoader.fromLanguageOrNull<LightWordEmbeddings>(sv)!! // We know this exists.. :)\n",
    "val sentEmbeddings = AverageSentenceEmbeddings(embeddings)\n",
    "\n",
    "sentEmbeddings.getSentenceEmbedding(listOf(\"Hej\", \"där\", \"borta\")).cols(0, 10) // once again reducing dimensions"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Type = FDRM , rows = 1 , cols = 10\n",
       "-2.3792E-02 -2.1282E-02 -4.2597E-02  3.0897E-02 -2.5323E-02 -9.7709E-02 -1.1211E-02  3.5199E-02  1.2124E-02 -8.6041E-03 \n"
      ]
     },
     "metadata": {
      "new_classpath": []
     },
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### USifSentenceEmbeddings\n",
    "This implementation is based on the paper _Unsupervised Random Walk Sentence Embeddings: A Strong but Simple Baseline_ by Kawin Ethayarajh, found on [aclweb.org](https://www.aclweb.org/anthology/W18-3012/).  \n",
    "This paper bases its work on Smooth-Inverse-Frequency (SIF) Embeddings (paper _A Simple but Tough-to-Beat Baseline for Sentence Embeddings_ found [here](https://openreview.net/forum?id=SyK00v5xx)). The difference being that this approach is _unsupervised_ while remaining even stronger. See the abstract:\n",
    "\n",
    "> Using a random walk model of text generation, Arora et al. (2017) proposed a strong baseline for computing sentence embeddings: take a weighted average of word embeddings and modify with SVD. This simple method even outperforms far more complex approaches such as LSTMs on textual similarity tasks. In this paper, we first show that word vector length has a confounding effect on the probability of a sentence being generated in Arora et al.’s model. We propose a random walk model that is robust to this confound, where the probability of word generation is inversely related to the angular distance between the word and sentence embeddings. Our approach beats Arora et al.’s by up to 44.4% on textual similarity tasks and is competitive with state-of-the-art methods. Unlike Arora et al.’s method, ours requires no hyperparameter tuning, which means it can be used when there is no labelled data.\n",
    "\n",
    "What has to be noted is that it requires more input than other `SentenceEmbeddings` see\n",
    "\n",
    "```kotlin\n",
    "import com.londogard.nlp.embeddings.sentence.*\n",
    "\n",
    "class USifSentenceEmbeddings(\n",
    "    val tokenEmbeddings: Embeddings,\n",
    "    private val wordProb: Map<String, Float>,\n",
    "    randomWalkLength: Int, // = n, ~11\n",
    "    val numCommonDiscourseVector: Int = 5 // = m, 0 should work. In practise max 5.\n",
    ") {\n",
    "    /** use it the same way as SentenceEmbeddings */\n",
    "}\n",
    "```\n",
    "\n",
    "Where `wordProb` is simply taken through the `WordFrequencies` util. E.g. `WordFrequencies.getAllWordFrequenciesOrNull(sv): Map<String, Float>?`.\n",
    "\n",
    "#### SentenceEmbeddings In Progress\n",
    "\n",
    "`TfIdfEmbeddings` & `SifEmbeddings`, the latter might be scrapped as `USif` is such a amazing work."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import com.londogard.nlp.wordfreq.WordFrequencies\n",
    "\n",
    "val usifEmbeddings = USifSentenceEmbeddings(embeddings, WordFrequencies.getAllWordFrequenciesOrNull(sv) ?: emptyMap(), 11)\n",
    "usifEmbeddings.getSentenceEmbedding(listOf(\"Hej\", \"där\", \"borta\")).cols(0, 10) // Also trunctating here"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Type = FDRM , rows = 1 , cols = 10\n",
       "-3.1692E-02 -2.7498E-02 -5.7967E-02  4.1469E-02 -3.2962E-02 -1.2680E-01 -1.5575E-02  4.6698E-02  1.4627E-02 -1.0986E-02 \n"
      ]
     },
     "metadata": {
      "new_classpath": []
     },
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenizers\n",
    "\n",
    "There currently exists three types of tokenizers all extending the same `interface`.\n",
    "\n",
    "```kotlin\n",
    "interface Tokenizer {\n",
    "    fun split(text: String): List<String>\n",
    "}\n",
    "```\n",
    "\n",
    "#### CharTokenizer\n",
    "\n",
    "Tokenizes a string into the chars."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import com.londogard.nlp.tokenizer.*\n",
    "\n",
    "CharTokenizer().split(\"hello, world!\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[h, e, l, l, o, ,,  , w, o, r, l, d, !]"
      ]
     },
     "metadata": {
      "new_classpath": []
     },
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SimpleTokenizer\n",
    "\n",
    "This is a word-tokenizer which splits out words based on a few different "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "SimpleTokenizer().split(\"hello, world!\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[hello, ,, world, !]"
      ]
     },
     "metadata": {
      "new_classpath": []
     },
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SentencePieceTokenizer\n",
    "The SentencePiece Tokenizer is a subword tokenizer which is Language Specific, through [BPEmb](https://nlp.h-its.org/bpemb/) we have 275 languages covered through Wikipedia. There exists model of the following vocab-sizes: `1000, 3000, 5000, 10_000, 25_000, 50_000, 100_000 & 200_000`. The larger vocab the less subwords are tokenized and more words.\n",
    "\n",
    "The SentencePiece model is the raw C++ from [Google](https://github.com/google/sentencepiece/) with a wrapper from [DJL (Amazon)](http://docs.djl.ai/extensions/sentencepiece/index.html). I'm usually very hesitant in adding native libraries (JNI) when working on JVM-projects but no-one can deny the power of SentencePiece and I haven't had the time to implement the algorithm myself.  \n",
    "This wrapper by DJL is very small (they provide a single dependency with only sentencepiece) and fits my philosophy of keeping dependencies and size at a low."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "SentencePieceTokenizer.fromLanguageSupportOrNull(sv)?.split(\"hej där borta, hur mår ni?\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[▁he, j, ▁där, ▁bor, ta, ,, ▁hur, ▁mår, ▁ni, ?]"
      ]
     },
     "metadata": {
      "new_classpath": []
     },
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stopwords\n",
    "\n",
    "The stopwords are taken from NLTK and are hosted directly on the GitHub. The object looks as follows:\n",
    "\n",
    "```kotlin\n",
    "object Stopwords {\n",
    "    fun isStopword(word: String, language: LanguageSupport): Boolean\n",
    "    fun stopwords(language: LanguageSupport): Set<String> // Throws if language does not support stopwords\n",
    "    fun stopwordsOrNull(language: LanguageSupport): Set<String>?\n",
    "}\n",
    "```\n",
    "\n",
    "This object has an internal cache which saves the previously loaded language. Use `Stopwords.stopwords` to simply retrieve the Stopwords and use them yourself as a `Set<String>`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import com.londogard.nlp.stopwords.Stopwords\n",
    "import com.londogard.nlp.utils.LanguageSupport.*\n",
    "\n",
    "val hej = Stopwords.isStopword(\"hej\", sv)\n",
    "val och = Stopwords.isStopword(\"och\", sv)\n",
    "\n",
    "\"'hej' is a stopword: $hej\\n'och' is a stopword: $och\""
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'hej' is a stopword: false\n",
       "'och' is a stopword: true"
      ]
     },
     "metadata": {
      "new_classpath": []
     },
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "Stopwords.stopwords(sv).take(3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[och, det, att]"
      ]
     },
     "metadata": {
      "new_classpath": []
     },
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "runCatching { Stopwords.stopwords(af) }.isSuccess // Stopwords does not support af"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "metadata": {
      "new_classpath": []
     },
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "Stopwords.stopwordsOrNull(af) == null"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {
      "new_classpath": []
     },
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### WordFrequencies\n",
    "\n",
    "The Word Frequencies are taken from `wordfreq.py` a library by [LuminosoInsight](https://github.com/LuminosoInsight/wordfreq/) and are hosted directly on the GitHub. The object looks as follows:\n",
    "\n",
    "```kotlin\n",
    "object WordFrequencies {\n",
    "   fun getAllWordFrequenciesOrNull(language: LanguageSupport, size: WordFrequencySize = WordFrequencySize.Largest): Map<String, Float>?\n",
    "   \n",
    "   fun wordFrequency(word: String, language: LanguageSupport, minimum: Float = 0f, size: WordFrequencySize): Float // Throws if language does not support wordfreq\n",
    "   fun wordFrequencyOrNull( word: String, language: LanguageSupport, minimum: Float = 0f, size: WordFrequencySize): Float?\n",
    "}\n",
    "```\n",
    "\n",
    "This object has an internal cache which saves the previously loaded language. Use `WordFrequencies.getAllWordFrequenciesOrNull` to simply retrieve the WordFrequencies and use them yourself as a `Map<String, Float>`.  \n",
    "Methods to recieve `zipfFrequencies` also exists."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import com.londogard.nlp.wordfreq.WordFrequencies\n",
    "\n",
    "val hej = WordFrequencies.wordFrequency(\"hej\", sv)\n",
    "val och = WordFrequencies.wordFrequency(\"och\", sv)\n",
    "\n",
    "\"WordFrequency of 'hej'=$hej and 'och'=$och\""
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "WordFrequency of 'hej'=2.9512093E-4 and 'och'=0.025118863"
      ]
     },
     "metadata": {
      "new_classpath": []
     },
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "val hej = WordFrequencies.zipfFrequency(\"hej\", sv)\n",
    "val och = WordFrequencies.zipfFrequency(\"och\", sv)\n",
    "\n",
    "\"ZipfFrequency of 'hej'=$hej and 'och'=$och\""
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ZipfFrequency of 'hej'=5.4700003 and 'och'=7.4"
      ]
     },
     "metadata": {
      "new_classpath": []
     },
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "val weird = WordFrequencies.wordFrequency(\"hraihaodjasmdiamo\", sv)\n",
    "val weirdOrNull = WordFrequencies.wordFrequencyOrNull(\"hraihaodjasmdiamo\", sv)\n",
    "\n",
    "\"WordFrequency of 'hraihaodjasmdiamo' (non-word) using `wordFrequency` $weird and using `wordFrequencyOrNull` $weirdOrNull\""
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "WordFrequency of 'hraihaodjasmdiamo' (non-word) using `wordFrequency` 0.0 and using `wordFrequencyOrNull` null"
      ]
     },
     "metadata": {
      "new_classpath": []
     },
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "runCatching { WordFrequencies.wordFrequency(\"hello\", af) }.isFailure // WordFrequencies does not support af"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {
      "new_classpath": []
     },
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "WordFrequencies.wordFrequencyOrNull(\"hello\", af) == null"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {
      "new_classpath": []
     },
     "execution_count": 19
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "WordFrequencies.getAllWordFrequenciesOrNull(sv)?.entries?.take(3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[är=0.037153527, det=0.031622775, att=0.026302677]"
      ]
     },
     "metadata": {
      "new_classpath": []
     },
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stemmer\n",
    "\n",
    "The stemmer uses [Snowballstem](http://snowballstem.org/) which is a small dependency with a wrapper.  \n",
    "If the stemmer is not supported by the called `LanguageSupport` it'll fall-back to the classic `PorterStemmer`.\n",
    "\n",
    "\n",
    "There exists two ways to call the stemmer currently."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import com.londogard.nlp.stemmer.Stemmer\n",
    "\n",
    "val stemmer = Stemmer(sv)\n",
    "stemmer.stem(\"katten\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "katt"
      ]
     },
     "metadata": {
      "new_classpath": []
     },
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "Stemmer.stem(\"katten\", sv)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "katt"
      ]
     },
     "metadata": {
      "new_classpath": []
     },
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trie\n",
    "\n",
    ":warning: Work In Progress :warning:\n",
    "\n",
    "Does work with a `vocab: Map<String, Int>`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vectorizer\n",
    "Vectorizers and Transformers\n",
    "\n",
    "A `Vectorizer` takes raw string input and outputs a vectorized format.\n",
    "A `Transformer` on the other way takes vectorized input and outputs a new vectorized format, _transformed_.\n",
    "\n",
    "The interface looks like the following:\n",
    "```kotlin\n",
    "interface Vectorizer<INPUT: Number, OUTPUT: Number> {\n",
    "    fun fit(input: List<List<String>>): Unit\n",
    "    fun transform(input: List<List<String>>): D2FloatArray\n",
    "    fun fitTransform(input: List<List<String>>): D2FloatArray\n",
    "}\n",
    "\n",
    "interface Transformer<INPUT: Number, OUTPUT: Number> {\n",
    "    fun fit(input: MultiArray<Float, D2>): Unit\n",
    "    fun transform(input: MultiArray<Float, D2>): MultiArray<Float, D2>\n",
    "    fun fitTransform(input: MultiArray<Float, D2>): MultiArray<Float, D2>\n",
    "}\n",
    "```\n",
    "\n",
    "Using this is very simple and all have the same way of interacting, simply replace the vectorizer for what you wish to use - here is a TF-IDF Vectorizer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val simpleTok = SimpleTokenizer()\n",
    "val simpleTexts = listOf(\"hejsan jag älskar sverige\", \"hej vad bra det är i sverige\", \"jag älskar sverige\", \"jag hatar norge\", \"norge hatar\", \"norge hatar\", \"norge hatar\")\n",
    "    .map(simpleTok::split)\n",
    "val tfidf = TfIdfVectorizer<Float>()\n",
    "\n",
    "val lhs = tfidf.fitTransform(simpleTexts)\n",
    "println(\"Vectorized: $lhs\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classifiers\n",
    "Classifiers predicts labels / classes based on a vectorized input.\n",
    "\n",
    "Interface\n",
    "```kotlin\n",
    "interface Classifier: BasePredictor<Int> {\n",
    "    fun fit(X: MultiArray<Float, D2>, y: D2Array<Int>)\n",
    "    fun predict(X: MultiArray<Float, D2>): D2Array<Int>\n",
    "}\n",
    "```\n",
    "\n",
    "TODO"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Regression\n",
    "Regressors predicts a continuous value based on a continous input.\n",
    "\n",
    "Interface\n",
    "```kotlin\n",
    "interface Regressor: BasePredictor<Int> {\n",
    "    fun fit(X: MultiArray<Float, D2>, y: D2Array<Float>)\n",
    "    fun predict(X: MultiArray<Float, D2>): D2Array<Float    >\n",
    "}\n",
    "```\n",
    "\n",
    "TODO"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SequenceClassifier\n",
    "A Sequence Classifier predicts a sequence of labels based on a sequence as a input, as an example it can be if a word is a verb, noun or something different.\n",
    "\n",
    "Currently only Hidden Markov Model (HMM) is supported.\n",
    "The interface is as follows:\n",
    "\n",
    "```kotlin\n",
    "interface SequenceClassifier<T : Number> {\n",
    "    // Using List<> as the input can be of different sizes between examples\n",
    "    fun fit(X: List<D1Array<T>>, y: List<D1Array<Int>>)\n",
    "    fun predict(X: List<D1Array<T>>): List<D1Array<Int>>\n",
    "}\n",
    "```\n",
    "\n",
    "TODO"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "pygments_lexer": "kotlin",
   "version": "1.5.0-dev-1206"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}