package com.londogard.nlp.tokenizer

object TokenizerSpecialTokens {
    const val Pad: Char = ' '
    const val AllCaps: Char = ' '
    const val Number: Char = ' '
    const val Upper: Char = ' '
    const val WordRepetition: Char = ' '
    const val CharRepetition: Char = ' '
    const val BOS: Char = ' '
    const val EOS: Char = ' '
    const val Space: Char = ' '
}